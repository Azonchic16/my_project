{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176ff41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3493/1233665598.py:54: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  def metrics(self, X: pd.DataFrame(), y: pd.Series()):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MyLogReg():\n",
    "    \n",
    "    def __init__(self, n_iter=10, learning_rate=0.1, weights=None, metric=None, reg=None,\n",
    "                l1_coef=0, l2_coef=0, sgd_sample=None, random_state=42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dynamic_l_r = not(isinstance(self.learning_rate, float))\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.metric_value = None\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}'\n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        n = len(X)\n",
    "        y = np.array(y)\n",
    "        m = len(X.columns)\n",
    "        X.insert(0, 'x0', [1] * n)\n",
    "        self.weights = np.ones(m + 1)\n",
    "        eps = 1e-15\n",
    "        alpha = self.dynamic_learning_rate()\n",
    "        random.seed(self.random_state)\n",
    "        X = X.reset_index()\n",
    "        del X['index']\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            prb = 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
    "            LogLoss = -(1 / n) * sum(y * np.log(prb + eps) + (1 - y) * np.log(1 - y + eps)) +\\\n",
    "                self.regularization()[0]\n",
    "            sample_index = self.sample(X)\n",
    "            if sample_index is None:\n",
    "                grad = (1 / n) * np.dot((prb - y), X) + self.regularization()[1]\n",
    "                self.weights = self.weights - alpha(i) * grad\n",
    "            else:\n",
    "                self.sgd(sample_index, X, y, alpha, i)\n",
    "        self.metric_value = self.metrics(X, y)\n",
    "        \n",
    "    def get_coef(self):\n",
    "        return np.mean(self.weights[1:])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not(len(self.weights) == len(X.columns)):\n",
    "            n = len(X)\n",
    "            X.insert(0, 'x0', [1] * n)\n",
    "        prb = 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
    "        return np.mean(prb)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not(len(self.weights) == len(X.columns)):\n",
    "            n = len(X)\n",
    "            X.insert(0, 'x0', [1] * n)\n",
    "        prb = 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
    "        y_pred = self.classification(prb, 0.5)\n",
    "        return int(sum(prb))\n",
    "    \n",
    "    def metrics(self, X: pd.DataFrame(), y: pd.Series()):\n",
    "        y = np.array(y)\n",
    "        n = len(X)\n",
    "        if not(len(self.weights) == len(X.columns)):\n",
    "            n = len(X)\n",
    "            X.insert(0, 'x0', [1] * n)\n",
    "        prb = 1 / (1 + np.exp(-np.dot(X, self.weights)))  # probabylyties\n",
    "        y_pred = self.classification(prb, 0.5)\n",
    "        if self.metric is None:\n",
    "            return None\n",
    "        elif self.metric  == 'roc_auc':\n",
    "            roc_auc = 0\n",
    "            positive = np.count_nonzero(y == 1)\n",
    "            negative = np.count_nonzero(y == 0)\n",
    "            for i in range(n):\n",
    "                roc_auc += sum(np.heaviside(y - y[i], 0) * np.heaviside(np.round(prb - prb[i], 10), 0.5))\n",
    "            roc_auc = roc_auc / (positive * negative)\n",
    "            return roc_auc\n",
    "        else:\n",
    "            TP = np.count_nonzero((prb == 1) & (prb == y))\n",
    "            TN = np.count_nonzero((prb == 0) & (prb == y))\n",
    "            FN = np.count_nonzero((prb == 0) & (prb != y))\n",
    "            FP = np.count_nonzero((prb == 1) & (prb != y))\n",
    "            accuracy = (TP + TN) / (TP + TN + FN + FP)\n",
    "            precision = TP / (TP + FP)\n",
    "            recall = TP / (TP + FN)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "            metrics = {'accuracy': accuracy, 'precision': precision,\n",
    "                   'recall': recall, 'f1': f1}\n",
    "            return metrics[self.metric]\n",
    "    \n",
    "    def classification(self, prb, level):\n",
    "        y_pred = np.copy(prb)\n",
    "        y_pred[y_pred > level] = 1\n",
    "        y_pred[y_pred <= level] = 0\n",
    "        return y_pred\n",
    "    \n",
    "    def get_best_score(self):\n",
    "        return self.metric_value\n",
    "    \n",
    "    def regularization(self):\n",
    "        l1 = self.l1_coef * sum(np.abs(self.weights))\n",
    "        l2 = self.l2_coef * sum((self.weights)**2)\n",
    "        gr1 = self.l1_coef * np.sign(self.weights)\n",
    "        gr2 = 2 * self.l2_coef * self.weights\n",
    "        regul = {'l1': l1,\n",
    "                 'l2': l2,\n",
    "                 'elasticnet': l1 + l2,\n",
    "                 None: 0}\n",
    "        regul_grad = {'l1': gr1,\n",
    "                      'l2': gr2,\n",
    "                      'elasticnet': gr1 + gr2,\n",
    "                      None: 0}\n",
    "        return (regul[self.reg], regul_grad[self.reg])\n",
    "    \n",
    "    def dynamic_learning_rate(self):\n",
    "        if self.dynamic_l_r:\n",
    "            return self.learning_rate\n",
    "        else:\n",
    "            a = lambda i: self.learning_rate\n",
    "            return a\n",
    "        \n",
    "    def sample(self, X):\n",
    "        if self.sgd_sample is None:\n",
    "            return None\n",
    "        elif isinstance(self.sgd_sample, int):\n",
    "            sample_rows_idx = random.sample(range(X.shape[0]), self.sgd_sample)\n",
    "            return sample_rows_idx\n",
    "        else:\n",
    "            n = len(X)\n",
    "            number = int(self.sgd_sample * n)\n",
    "            sample_rows_idx = random.sample(range(X.shape[0]), number)\n",
    "            return sample_rows_idx\n",
    "        \n",
    "    def sgd(self, sample_index, X, y, alpha, i):\n",
    "        X_sample = X.iloc[sample_index]\n",
    "        n1 = len(X_sample)\n",
    "        prb = 1 / (1 + np.exp(-np.dot(X_sample, self.weights)))\n",
    "        y1 = [y[k] for k in sample_index]\n",
    "        grad = (1 / n1) * np.dot((prb - np.array(y1)), X_sample) + self.regularization()[1]\n",
    "        self.weights = self.weights - alpha(i) * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260aa1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.35 34.    5.    2.   54.   32.   43.    2.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1.3543453,34,5,2,54,32,43,2])\n",
    "b = np.array([0,34,5,2,9,32,43,8])\n",
    "print(np.round(a, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
